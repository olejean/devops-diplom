# Дипломный проект по курсу DevOps от SkillFactory
***

[![Build](https://img.shields.io/badge/Build-stable-!)](https://hub.docker.com/layers/olejean/app-diplom/v1.0.5/images/sha256-523a57783bfd398b085bbca356e952e6b1d65ccc95c5c23fd467223fe702dd4b?context=repo)
#####  В проекте используется ряд иструменнов с открытым исходным кодом :
- [Terraform](https://www.terraform.io/) -  инструмент управления состоянием облачной инфраструктуры.
- [Ansible](https://www.ansible.com/) - Управление конфигурацией и развертывание приложений, на множество серверов.
- [GitHub](https://github.com/olejean/devops-diplom) - GitHub — крупнейший веб-сервис для хостинга IT-проектов и их совместной разработки.
- [CI/CD - GitLab](https://gitlab.com) - GitLab — это инструмент для хранения и управления репозиториями Git.
- [Kubernetes](https://kubernetes.io/) - Праграмное обеспечение для автоматизации развёртывания, масштабирования и управления контейнеризированными приложениями.
- [Prometheus](https://prometheus.io/) - Prometheus — система мониторинга и оповещений, хранящая и обрабатывающая метрики, собираемые из экспортеров в Time Series Database (TSDB)
- [Grafana-Loki](https://grafana.com/oss/loki/) - Логирование (журналирование) централизованный сбор логов
- [Zabbix](https://www.zabbix.com/ru/) - Предназначен для мониторинга и отслеживания производительности и доступности сетевых серверов
***
## Задание 
#### Задание разделено на три этапа:
___1. Опишите инфраструктуру будущего проекта в виде кода с инструкциями по развертке, нужен кластер Kubernetes и служебный сервер (будем называть его srv).___
- Нам нужно три сервера: два сервера в одном кластере Kubernetes: 1 master и 1 app 
- сервер srv для инструментов мониторинга, логгирования и сборок контейнеров. 
- Описывать инфраструктуру мы будем, конечно, в Terraform.
- Нужно реализовать возможность установки на сервер всех необходимых нам настроек и пакетов через Ansible

___2. Соберем и задеплоим приложение из нашего Git в созданный кластер Kubernetes. Разделим всё на шаги.___
- Клонируем репозиторий, собираем его на сервере srv. Исходники простого приложения можно [взять здесь](https://github.com/vinhlee95/django-pg-docker-tutorial). Это простое приложение на Django с уже написанным Dockerfile. Приложение работает с PostgreSQL, в самом репозитории уже есть реализация docker-compose — её можно брать за референс при написании Helm-чарта.
Необходимо склонировать репозиторий выше к себе в Git и настроить пайплайн с этапом сборки образа и отправки его в любой docker registry. Для пайплайнов можно использовать GitLab, Jenkins или GitHub Actions — кому что нравится. Рекомендуем GitLab.

- Описываем приложение в виде конфигов в Helm-чарте. Стоит хранить данные в БД с помощью PVC в Kubernetes.
- Описываем стадию деплоя в Helm.
Настраиваем деплой стадию пайплайна. Применяем Helm-чарт в наш кластер. Нужно сделать так, чтобы наше приложение разворачивалось после сборки в Kubernetes и было доступно по бесплатному домену или на IP-адресе с выбранным портом.
Для деплоя должен использоваться свежесобранный образ. По возможности нужно реализовать сборку из тегов в Git, где тег репозитория в Git будет равен тегу собираемого образа. Чтобы создание такого тега запускало пайплайн на сборку образа c таким именем testapp:2.0.3

___3. Настройка сборки логов.___
- Выберите инструмент, с помощью которого такой функционал можно предоставить. Нужно собирать логи работы пода приложения. Хранить это всё можно либо в самом кластере Kubernetes, либо на srv-сервере.

- Выбор метрик для мониторинга.
Так, теперь творческий этап. Допустим, наше приложение имеет для нас некоторую важность. Мы бы хотели знать, когда пользователь не может на него попасть — время отклика, сертификат, статус код и так далее. Выберите метрики и инструмент, с помощью которого будем отслеживать его состояние.
Также мы хотели бы знать, когда место на srv-сервере подходит к концу.
Важно! Весь мониторинг должен находиться на srv-сервере, чтобы в случае падения кластера мы все равно могли узнать об этом.

- Настройка дашборда.
Ко всему прочему хотелось бы и наблюдать за метриками в разрезе времени. Для этого мы можем использовать Grafana и Zabbix — что больше понравилось.

- Алертинг.
А теперь добавим уведомления в наш любимый мессенджер, точнее в ваш любимый мессенджер. Обязательно протестируйте отправку уведомлений. Попробуйте «убить» приложение самостоятельно, и засеките время от инцидента до получения уведомления. Если время адекватное, то можно считать, что вы справились с этим проектом!
Самое время положить все возможные конфигурации в Git-репозиторий, если вы этого ещё не сделали.
***


## Решение 
#### Разворачиваем инфрастуктуру как код (iaC) через Terraform в Yandex Cloud

- Разворачиваем  три инстанса.  Два сервера в одном кластере Kubernetes: 1 master (Managed Service for Kubernetes) и одна нода (Node) app 
-  Сервер srv для инструментов мониторинга, логгирования и сборок контейнеров.
#### installation
Основной файл [README.md](https://github.com/olejean/devops-diplom/blob/main/README.md)   находится в этом Репозиторий.
Клонируем репозиторий  https://github.com/olejean/devops-diplom.git Переходим в папку terraform и запускаем terraform apply  -var-file=secret.tfvars
Все секретные данные находится в отдельно файле и в git  не попали. Готово наша инфрастуктура развернута.


```sh
git pull https://github.com/olejean/devops-diplom.git
cd ~/devops-diplom/terraform/
terraform apply  -var-file=secret.tfvars
```

Устанавливаем Ansible  на сервер srv, что бы с него можно было автоматизировать установку приложений. переходим а папку ansible  и запускаем.
___Устанавливаем приложения необходимые для работы___
- Kubectl
- Helm
- Prometheus
- Docker
- net-tools
- gitlab-runner
```
apt install ansible
cd ~/devops-diplom/ansible
ansible-playbook playbook_all.yml -i hosts
```
__Отдельно установим в кластер:__
- Устанавливаем ALB Ingress Controller 
- Создадим DNS  зону в yandex cloud на домен olejean.ru (предварительно делигировав dns от -   регистратора к YC) Приложение будет доступно по адресу app.infra.olejean.ru
    

    

### 2. Cобираем, деплоим наше приложение  настраиваем CI/CD
Клонируем репозиторий, собираем его на сервере srv.

Регистрируем gitlab-runner  на сервере srv
```
sudo gitlab-runner register --url https://gitlab.com/ --registration-token <token>
```    

Настраиваем доступ к docker-hub для хранения образов созданных с помошью пайплана gitlab-ci
```
docker login -u olejean
```

Клонируем наше приложение на сервер srv
```
git clone https://gitlab.com/olejean/diplom-app2.git

```
__Настраиваем CI/CD - Описываем .gitlab-ci.yml__
- Первая стадия будет build - будет собирать образ и отправлять его на dockerhub olejean/app-diplom:tagname
- вторая стадия deploy  будет разворичивать приложение через Help  на наш кластер Kebernetes.



__Подключаем сертификаты__
- Воспользуюсь сервисом Yandex Certificate Manager.
- Создадим сертификат на доменное имя,  для приложения app.infra.olejean.ru

### 3. Разворачивакем мониторинг 
- Клонируем репозиторий с мониторингом и запускаем сначала через docker compose - prometheus  
```
git clone git@github.com:olejean/monitoring-diplom.git
cd prometheus/
docker compose up -d
```

- Далее развернем grafana-loki Так как у нас есть grafanf добавим в нее loki  
- Отдельно установим в кластер kubernetes promtail для отправки логов через Helm
```
helm install promtail --create-namespace promtail ./promtail
```



